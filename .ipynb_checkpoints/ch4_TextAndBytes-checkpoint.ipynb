{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 텍스트와 바이트\n",
    "파이썬 3부터는 인간이 사용하는 텍스트 문자열과 원시 바이트 시퀀스를 엄격히 구분하기 시작했다. 암묵적으로 바이트 시퀀스를 유니코드 텍스트로 변환하는 것은 과거의 일이다. 이 장에서는 유니코드 문자열, 이진 시퀀스, 그리고 이 둘 간의 변환에 사용되는 인코딩에 대해 설명한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 문자열\n",
    "유니코드 표준은 문자의 단위 원소와 특정 바이트 표현을 명확히 구분한다.\n",
    "<li> 문자의 단위 원소(코드 포인트, code point)는 10진수 0에서 1,114,111까지의 숫자이며, 유니코드 표준에서는 'U+' 접두사를 붙여 4자리에서 6자리 사이의 16진수로 표현한다. 예를 들어 A라는 문자는 코드 포인트 U+0041에, 유로화 기호는 U+20AC에 음악에서 사용하는 높은음 자리표는 U+1DD11E에 할당되어 있다.</li>\n",
    "<li> 문자를 표현하는 실제 바이트는 사용하는 인코딩에 따라 달라진다. 인코딩은 코드 포인트를 바이트 시퀀스로 변환하는 알고리즘이다. 문자 A(U+0041)에 대한 코드 포인트는 UTF-8 인코딩에서는 1바이트\\x41, UTF-16LE 인코딩에서는 2바이트 \\x41\\x00으로 인코딩된다. 그리고 유로화 기호(U+20AC)는 UTF-8에서는 3바이트 \\xe2\\x82\\xac로 UTF-16LE에서는 2바이트 \\xac\\x20으로 인코딩된다.</li>\n",
    "\n",
    "코드 포인트를 바이트로 변환하는 것을 인코딩, 바이트를 코드 포인트로 변환하는 것을 디코딩이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "b'caf\\xc3\\xa9'\n",
      "5\n",
      "café\n"
     ]
    }
   ],
   "source": [
    "# 예제 4-1 인코딩과 디코딩\n",
    "s = 'café'\n",
    "print(len(s))\n",
    "b = s.encode('utf8') # 바이트로 암호화\n",
    "print(b)             # byte 문자는 접두사 b로 시작한다.\n",
    "print(len(b))        # byte형인 b는 다시 바이트로 구성된다. e가 UTF-8에서 두 바이트로 인코딩되기 때문이다.\n",
    "print(b.decode('utf8')) # 다시 str로 해석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 바이트에 대한 기본 지식\n",
    "이진 시퀀스를 위해 사용되는 내장 자료형은 byte와 bytearray이다. 각 항목은 0에서 255사이의 정수로 구성된다.\n",
    "\n",
    "※ my_bytes[0]은 int형을 반환하지만, my_bytes[:1]은 길이가 1인 bytes 객체를 반환한다. s[0] == s[:1]이 되는 시퀀스는 str이 유일하다. 실용적이기는 하지만 str의 이런 동작은 예외적인 것이다. 그 외 모든 시퀀스는 s[i]는 항목 하나를 s[i:i+1]은 안에 s[i] 항목을 가진 동일한 자료형의 시퀀스를 반환한다.\n",
    "\n",
    "각 바이트는 값에 따라 다음과 같이 세가지 형태로 출력된다. \n",
    "+ 화면에 출력 가능한 아스키 문자(공백에서 물결표(~)까지)는 아스키 문자 그대로 출력\n",
    "+ 탭, 개행문자, 캐리지 리턴, 백슬래시는 이스케이프 시퀀스(\\t, \\n, \\r, \\\\\\\\)로 출력\n",
    "+ 그 외의 값은 널 바이트를 나타내는 /x00처럼 16진수 이스케이프 시퀀스로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'caf\\xc3\\xa9'\n",
      "99\n",
      "b'c'\n",
      "bytearray(b'caf\\xc3\\xa9')\n",
      "bytearray(b'\\xa9')\n"
     ]
    }
   ],
   "source": [
    "cafe = bytes('café', encoding = 'utf_8') # str에 인코딩을 지정해서 바이트를 생성\n",
    "print(cafe)\n",
    "print(cafe[0]) # 각 바이트는 0~255 수로 구성되어 있다.\n",
    "print(cafe[:1]) # 바이트를 슬라이싱하면 길이가 1이더라도 바이트가 반환된다.\n",
    "cafe_arr = bytearray(cafe)\n",
    "print(cafe_arr)\n",
    "print(cafe_arr[-1:]) # 바이트 배열 슬라이스도 바이트 배열이 반환된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bytes와 bytearray는 str의 format()과 format_map()를 제외하고 모든 메서드를 지원하며, casefold(), isdecimal(), isidentifier(), isnumeric(), isprintable(), encode() 등 유니코드 데이터에 관련된 메서드를 추가로 지원한다. 따라서 endswith(), replace(), strip(), translate(), upper() 등의 메서드를 str이 아닌 bytes에서도 사용할 수 있다. 또한 re모듈에서 제공하는 정규표현식 함수도 사용할 수 있다.\n",
    "\n",
    "이진 시퀀스는 fromhex( )라는 str에 없는 클래스 메서드도 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'1K\\xce\\xa9'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes.fromhex('31 4B CE A9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bytes나 bytearray 객체를 생성하는 방법은 다음과 같다.\n",
    "+ str과 encoding 키워드 인수\n",
    "+ 0에서 255사이의 값을 제공하는 반복 가능형\n",
    "+ bytes, bytearray, memoryview, array.array 등 버퍼 프로토콜을 구현하는 객체, 이 메서드를 사용하면 원본 객체의 바이트를 복사해서 바이트 시퀀스를 새로 생성한다.\n",
    "\n",
    "버퍼 등의 객체로부터 이진 시퀀스를 생성하는 방법은 저수준 연산으로, 형변환이 필요할 수 있다(예제 4-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array('h', [-2, -1, 0, 1, 2])\n",
      "b'\\xfe\\xff\\xff\\xff\\x00\\x00\\x01\\x00\\x02\\x00'\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 예제 4-3 배열의 원시 데이터에서 bytes 초기화하기\n",
    "import array\n",
    "numbers = array.array('h', [-2, -1, 0, 1, 2])\n",
    "print(numbers)\n",
    "octets = bytes(numbers)\n",
    "print(octets)\n",
    "print(len(octets)) # array 'h'는 short int(16비트 이므로) 5개 글자는 10 bytes이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 구조체와 메모리 뷰\n",
    "struct 모듈은 패킹된 바이트를 다양한 형의 필드로 구성된 튜플로 분석하고, 이와 반대로 튜플을 패킹된 바이트로 변환하는 함수를 제공한다. struct는 bytes, bytearray, memoryview 객체와 함께 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'GIF89a\\xd5\\x02\\xe9\\x01'\n",
      "(b'GIF', b'89a', 725, 489)\n"
     ]
    }
   ],
   "source": [
    "# [예제 4-4] memoryview와 struct를 사용해서 GIF 이미지 헤더 조사하기\n",
    "import struct\n",
    "fmt = '<3s3sHH' # <는 리틀엔디언, 3s3s는 3바이트 시퀀스 두 개, HH는 16비트 정수 두 개를 나타낸다.\n",
    "with open('wave.gif', 'rb') as fp:\n",
    "    img = memoryview(fp.read()) # 메모리에 로딩된 파일 내용으로부터 memoryview를 생성한다.\n",
    "    \n",
    "header = img[:10] # 메모리뷰를 슬라이싱하면 바이트를 복사하지 않고 새로운 memoryview 객체를 반환한다.\n",
    "                  # mmap 모듈을 사용해서 이미지를 메모리 맵 파일(mmap)로 열면 더 적은 메모리를 사용한다. \n",
    "print(bytes(header))\n",
    "print(struct.unpack(fmt, header))\n",
    "del header\n",
    "del img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 기본 인코더/디코더\n",
    "텍스트를 바이트로 혹은 바이트를 텍스트로 변환하기 위해 파이썬 배포본에는 100여 개의 코덱이 포함되어 있다. 코덱은 open(), str.encode(), bytes.decode() 등의 함수를 호출할 때 encoding 인수에 전달해서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin_1/tb'EL nino'\n",
      "utf_8/tb'EL nino'\n",
      "utf_16/tb'\\xff\\xfeE\\x00L\\x00 \\x00n\\x00i\\x00n\\x00o\\x00'\n"
     ]
    }
   ],
   "source": [
    "# [예제 4-5] 전혀 다른 바이트 시퀀스를 만드는 세 개의 코덱으로 인코딩한 문자열\n",
    "for codec in ['latin_1', 'utf_8', 'utf_16']:\n",
    "    print(codec, 'EL nino'.encode(codec), sep='/t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 인코딩/디코딩 문제\n",
    "유니코드 에러가 발생할 때는 먼저 예외의 정확한 유형을 알아내야 한다. 문제를 해결하려면 먼저 인코딩 에러가 UnicodeEncodeErr인지 혹은 UnicodeDecodeErr인지 SyntaxError 등 다른 에러인지 구체적인 유형을 알아내야 한다. \n",
    "#### 4.4.1 UnicodeEncodeErr\n",
    "대부분 UTF가 아닌 코덱은 유니코드 문자가 대상 인코딩에 모두 정의되어 있지 않기 때문에 errors 인수에 별도의 처리기를 지정하지 않는 한 UnicodeEncodeErr를 발생시킬 여지가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\xe3' in position 1: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0fdacefe200f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf_8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf_16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cp437'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 인코딩 시 ã에 매칭되는 byte가 정의되어 있지 않은 코덱을 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Apps/miniconda3/envs/tensorflow/lib/python3.6/encodings/cp437.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, input, errors)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\xe3' in position 1: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "city = 'São Paulo'\n",
    "city.encode('utf_8')\n",
    "city.encode('utf_16')\n",
    "city.encode('cp437') # 인코딩 시 ã에 매칭되는 byte가 정의되어 있지 않은 코덱을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(city.encode('cp437', errors='ignore'))  # 에러 부분을 건너뜀 (일반적으로 좋지 않음)\n",
    "print(city.encode('cp437', errors='replace')) # 인코딩 할 수 없는 문자를 ?표시로 치환\n",
    "print(city.encode('cp437', errors='xmlcharrefreplace')) # 인코딩 할 수 없는 문자를 XML 개체로 치환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 UnicodeDecodeErr\n",
    "모든 바이트가 아스키 문자가 될 수 없으며, 모든 바이트 시퀀스가 UTF-8 이나 UTF-16 문자가 되는 것은 아니므로 이 때 UnicodeDecodeError가 발생한다. 그렇지만 'cp1252', 'iso8859_1', 'koi8_r' 등 많은 레거시 8비트 코덱은 무작위 비트 배열에 대해서도 에러를 발생시키지 않고 잘못된 바이트 스트림으로 디코딩 할 수 있어 주의가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montréal\n",
      "Montrιal\n",
      "MontrИal\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 5: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e90095dbd4da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moctets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iso8859_7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moctets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'koi8_r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moctets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf_8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 5: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "octets = b'Montr\\xe9al'\n",
    "print(octets.decode('cp1252'))\n",
    "print(octets.decode('iso8859_7'))\n",
    "print(octets.decode('koi8_r'))\n",
    "print(octets.decode('utf_8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montr�al\n"
     ]
    }
   ],
   "source": [
    "print(octets.decode('utf_8', errors='replace')) # �는 알 수 없는 문자를 표현하기 위해 사용하는 공식 유니코드 치환 문자이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 잘못된 코덱으로 인코딩된 모듈을 로딩할 때 발생하는 SyntaxError\n",
    "파이선 3부터는 UTF-8을 소스 코드의 기본 인코딩 방식으로 사용한다. 따라서 인코딩 선언없이 비 UTF-8로 인코딩된 py모듈을 로딩하면 에러가 발생한다. 다만 파일 제일 처음에 코덱을 명시하여 문제를 해결할 수 있다.\n",
    "\n",
    "파이썬 소스 코드는 더 이상 아스키 문자에 구애받지 않고 모든 플랫폼에서 UTF-8 인코딩을 기본적으로 사용하고 있으므로 cp1252와 같은 레거시 코덱으로 인코딩된 소스 코드는 UTF-8로 변환하는 것이 좋으며 굳이 coding 주석을 사용할 필요가 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá Mundo\n"
     ]
    }
   ],
   "source": [
    "# coding: cp1252\n",
    "\n",
    "print('Olá Mundo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4 바이트 시퀀스의 인코딩 방식을 알아내는 방법\n",
    "Chardet는 규칙과 제한을 고려하여 30가지 인코딩 방식을 추정한다. chardetect라는 명령행 유틸리티도 포함하고 있다.\n",
    "\n",
    "`$ chardetect 04-text-bypte.asciidoc\n",
    "04-text-bypte.asciidoc: utf-8 with confidence 0.99`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chardet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ca4c9633668e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'São Paulo'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchardet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf_8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchardet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf_16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chardet'"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "city = 'São Paulo'\n",
    "print(chardet.detect(city.encode('utf_8')))\n",
    "print(chardet.detect(city.encode('utf_16')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.5 BOM : Byte Order Mark\n",
    "<a href=\"https://ko.wikipedia.org/wiki/%EC%97%94%EB%94%94%EC%96%B8\">엔디언(Endianness)</a>은 컴퓨터의 메모리와 같은 1차원의 공간에 여러 개의 연속된 대상을 배열하는 방법을 뜻하며, 바이트를 배열하는 방법을 특히 바이트 순서(Byte order)라 한다. 엔디언은 보통 큰 단위가 앞에 나오는 빅 엔디언(Big-endian)과 작은 단위가 앞에 나오는 리틀 엔디언(Little-endian)으로 나눌 수 있으며, 두 경우에 속하지 않거나 둘을 모두 지원하는 것을 미들 엔디언(Middle-endian)이라 부르기도 한다. 엔디언은 한 바이트 이상을 워드로 사용하는 UTF-16, UTF-32에서만 영향을 준다. UTF-8은 엔디언 특성과 상관없이 동일한 바이트 시퀀스를 생성하므로 BOM이 필요없으나 윈도우 APP 중에서는 BOM(b'\\xef\\xbb\\xbf')을 붙이는 경우가 있으므로 참고해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xfeE\\x00L\\x00 \\x00n\\x00i\\x00n\\x00o\\x00'\n"
     ]
    }
   ],
   "source": [
    "u16 = 'EL nino'.encode('utf_16')\n",
    "print(u16) # b'\\xff\\xfe' 는 바이트 순서 표시(BOM)로 리틀 엔디언(little endian)을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255, 254, 69, 0, 76, 0, 32, 0, 110, 0, 105, 0, 110, 0, 111, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(u16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "255, 254는 리틀엔디언을 의미하는 b'\\xff\\xfe' 이며, 69, 0은 첫 문자 'E'를 의미한다. 빅엔디언이라면 0, 69 순으로 반대로 표현된다. 엔디언 문제는 한 바이트 이상을 워드로 사용하는 UTF-16과 UTF-32에만 영향을 준다. UTF-8은 엔디언과 관계없이 동일한 바이트 시퀀스를 생성하므로 BOM이 필요없다는 장점이 있으나 윈도우 프로그램 일부는 BOM을 명시하기도 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'E\\x00L\\x00 \\x00n\\x00i\\x00n\\x00o\\x00'\n",
      "b'\\x00E\\x00L\\x00 \\x00n\\x00i\\x00n\\x00o'\n"
     ]
    }
   ],
   "source": [
    "# 엔디언을 명시하는 utf_16le, utf_16be은 BOM을 붙이지 않는다.\n",
    "u16 = 'EL nino'.encode('utf_16le') # BOM을 붙이지 않음\n",
    "print(u16) \n",
    "u16 = 'EL nino'.encode('utf_16be') # BOM을 붙이지 않음\n",
    "print(u16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 텍스트 파일 다루기\n",
    "유니코드 샌드위치란 파일을 읽기 위해 여는 등 입력할 때 가능하면 빨리 bytes를 str로 변환해야 하며, 가능한 늦게 str을 bytes로 인코딩한다. 또한 여러 상황의 컴퓨터에서 실행을 감안하여 인코딩 기본값에 의존하지말고 endoding 인수를 명시적으로 지정하는 것이 좋다. ![alt text](unicodesandwich.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cafè'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [예제 4-9] cp1252를 기본으로 사용하는 윈도우에서는 잘못된 인코딩을 한다.\n",
    "open('cafe.txt', 'w', encoding = 'utf_8').write('cafè') # utf_8로 기록\n",
    "open('cafe.txt').read() # 윈도우에서는 open('cafe.txt', 'r', encoding = 'cp1252').read()로 동작함(기본값 cp1252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='cafe.txt' mode='w' encoding='utf_8'>\n",
      "4\n",
      "os.stat_result(st_mode=33188, st_ino=6182291, st_dev=2053, st_nlink=1, st_uid=1000, st_gid=1000, st_size=5, st_atime=1543074434, st_mtime=1543074434, st_ctime=1543074434)\n",
      "<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='UTF-8'>\n",
      "UTF-8\n",
      "cafè\n",
      "<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='utf_8'>\n",
      "utf_8\n",
      "cafè\n",
      "<_io.BufferedReader name='cafe.txt'>\n",
      "b'caf\\xc3\\xa8'\n"
     ]
    }
   ],
   "source": [
    "# [예제 4-10], 예제 4-9의 확장\n",
    "fp = open('cafe.txt', 'w', encoding = 'utf_8')\n",
    "print(fp)\n",
    "print(fp.write('cafè')) # 4자를 입력\n",
    "fp.close()\n",
    "\n",
    "import os\n",
    "print(os.stat('cafe.txt')) # è가 2바이트로 변환되기 때문에 st_size가 5bytes로 나옴 \n",
    "\n",
    "fp2 = open('cafe.txt') # TextIOWrapper 객체가 반환됨\n",
    "print(fp2)\n",
    "print(fp2.encoding)\n",
    "print(fp2.read()) # 잘못된 값이 출력됨\n",
    "\n",
    "fp3 = open('cafe.txt', encoding='utf_8') # TextIOWrapper 객체가 반환됨\n",
    "print(fp3)\n",
    "print(fp3.encoding)\n",
    "print(fp3.read()) # 정상값이 값이 출력됨\n",
    "\n",
    "fp4 = open('cafe.txt', mode='rb') # TextIOWrapper가 아닌 BufferedReader객체가 반환됨\n",
    "print(fp4)\n",
    "print(fp4.read()) # bytes가 반환됨\n",
    "                  # 인코딩 방식을 알아내기 위해 파일 내용을 분석하는 경우가 아니라면 텍스트 파일을\n",
    "                  # 이진 모드로 열지 않는 것이 좋다. 인코딩 방식을 알아낼 때도 직접하는 것보다 Chardet 모듈을 사용하는 것이 좋다.\n",
    "                  # 일반적으로 래스터 이미즈 등 이진 파일을 열 때만 이진 모드를 사용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코딩 방식을 알아내기 위해 파일 내용을 분석하는 경우가 아니라면 텍스트 파일을 이진 모드로 열지 않는 것이 좋으며, 인코딩 방식을 확인할 때도 Chardet 모듈을 사용하는 것이 좋다. 일반적으로 래스터 이미지 등 이진 파일을 열 때만 이진 모드를 사용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 기본 인코딩 설정\n",
    "가장 중요한 인코딩 설정은 locale.getpreferredenciding() 함수가 반환하는 설정이다. 리눅스/GNU/OS X는 수년간 기본적으로 모든 인코딩이 UTF-8로 설정되었지만, 윈도우에서는 동일한 시스템 안에 여러 인코딩이 사용되므로 기본 인코딩에 의존하지 않는 것이 가장 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " locale.getpreferredencoding() -> 'UTF-8'\n",
      "                 type(my_file) -> <class '_io.TextIOWrapper'>\n",
      "              my_file.encoding -> 'UTF-8'\n",
      "           sys.stdout.encoding -> 'UTF-8'\n",
      "           sys.stdout.isatty() -> False\n",
      "            sys.stdin.encoding -> 'UTF-8'\n",
      "           sys.stderr.isatty() -> False\n",
      "           sys.stderr.encoding -> 'UTF-8'\n",
      "      sys.getdefaultencoding() -> 'utf-8'\n",
      "   sys.getfilesystemencoding() -> 'utf-8'\n"
     ]
    }
   ],
   "source": [
    "# [예제 4-11] 리눅스 인코딩 기본값 확인\n",
    "import sys, locale\n",
    "\n",
    "expressions = \"\"\"\n",
    "locale.getpreferredencoding()\n",
    "type(my_file)\n",
    "my_file.encoding\n",
    "sys.stdout.encoding\n",
    "sys.stdout.isatty()\n",
    "sys.stdin.encoding\n",
    "sys.stderr.isatty()\n",
    "sys.stderr.encoding\n",
    "sys.getdefaultencoding()\n",
    "sys.getfilesystemencoding()\n",
    "\"\"\"\n",
    "\n",
    "my_file = open('dummy', 'w')\n",
    "\n",
    "for expression in expressions.split():\n",
    "    value = eval(expression)\n",
    "    print(expression.rjust(30), '->', repr(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " locale.getpreferredencoding() -> 'UTF-8'\n",
      "                 type(my_file) -> <class '_io.TextIOWrapper'>\n",
      "              my_file.encoding -> 'UTF-8'\n",
      "           sys.stdout.encoding -> 'UTF-8'\n",
      "           sys.stdout.isatty() -> False\n",
      "            sys.stdin.encoding -> 'UTF-8'\n",
      "           sys.stderr.isatty() -> False\n",
      "           sys.stderr.encoding -> 'UTF-8'\n",
      "      sys.getdefaultencoding() -> 'utf-8'\n",
      "   sys.getfilesystemencoding() -> 'utf-8'\n"
     ]
    }
   ],
   "source": [
    "# [예제 4-11] window 인코딩 기본값 확인\n",
    "import sys, locale\n",
    "\n",
    "expressions = \"\"\"\n",
    "locale.getpreferredencoding()\n",
    "type(my_file)\n",
    "my_file.encoding\n",
    "sys.stdout.encoding\n",
    "sys.stdout.isatty()\n",
    "sys.stdin.encoding\n",
    "sys.stderr.isatty()\n",
    "sys.stderr.encoding\n",
    "sys.getdefaultencoding()\n",
    "sys.getfilesystemencoding()\n",
    "\"\"\"\n",
    "\n",
    "my_file = open('dummy', 'w')\n",
    "\n",
    "for expression in expressions.split():\n",
    "    value = eval(expression)\n",
    "    print(expression.rjust(30), '->', repr(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 제대로 비교하기 위해 유니코드 정규화하기\n",
    "유니코드에는 결합 문자가 있기 때문에 문자열 비교가 간단하지 않다. 앞 문자에 연결되는 발음구별 기회(diacritical mark)는 인쇄할 때 앞 문자와 하나로 결합되어 출력된다. 예를 들어 'cafè'는 네 개나 다섯 개의 코드 포인트를 이용해서 두 가지 방식으로 표현할 수 있으며 결과는 동일하지만 서로 동일하지 않다고 판단하는 문제가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "café café\n",
      "5 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 'café'\n",
    "s2 = 'cafe\\u0301' # \\u0301는 앞문자와 결합하는 combining acute accent\n",
    "print(s1, s2)\n",
    "print(len(s1), len(s2))\n",
    "s1 == s2 # 결과는 같으나 동일하지 않다고 판단? (책과 달리 잘 되는 것 같기도 하다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 문제를 해결하려면 unicodedata.normalize() 함수가 제공하는 유니코드 정규화를 이용해야 한다. 이 함수의 첫 인수는 정규화 방식인 NFC, NFD, NFKC, NFKD 중 하나여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5\n",
      "4 4\n",
      "5 5\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "s1 = 'cafè'\n",
    "s2 = 'cafe\\u0301'\n",
    "print(len(s1), len(s2))\n",
    "print(len(normalize('NFC', s1)), len(normalize('NFC', s2))) # W3C가 추천하는 정규화 형식이기도 하다\n",
    "print(len(normalize('NFD', s1)), len(normalize('NFD', s2)))\n",
    "\n",
    "print(len(normalize('NFC', s1)) == len(normalize('NFC', s2)))\n",
    "print(len(normalize('NFD', s1)) == len(normalize('NFD', s2)))\n",
    "\n",
    "# NFKC, NFKD는 정보를 왜곡할 수 있지만 검색 및 색인 생성을 위한 편리한 중간 형태를 생성할 수 있다.\n",
    "# 다만 영구 저장할 때는 데이터가 손실될 수 있기 때문에 사용을 삼가해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 케이스 폴딩\n",
    "본질적으로 케이스 폴딩은 모든 텍스트를 소문자로 변환하는 연산이며 유니코드인 경우 약간의 변환을 동반한다. str.casfold() 메서드를 이용한다. 파이선 3.4에는 str.casefold()와 str.lower()가 서로 다른 문자를 반환하는 코드 포인트가 116개 있다. 이 수치는 유니코드 6.3에서 명명된 110,122개 문자의 0.11%에 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GREEK SMALL LETTER MU\n",
      "μ\n",
      "LATIN SMALL LETTER SHARP S\n",
      "ss\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize, name\n",
    "micro = 'μ'\n",
    "print(name(micro))\n",
    "micro_cf = micro.casefold()\n",
    "print(micro_cf)\n",
    "\n",
    "eszett = 'ß'\n",
    "print(name(eszett))\n",
    "eszett_cf = eszett.casefold()\n",
    "print(eszett_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 정규화된 텍스트 매칭을 위한 유틸리티 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "def nfc_equal(str1, str2):\n",
    "    return normalize('NFC', str1) == normalize('NFC', str2)\n",
    "\n",
    "def fold_equal(str1, str2):\n",
    "    return (normalize('NFC', str1).casefold() ==\n",
    "            normalize('NFC', str1).casefold())\n",
    "\n",
    "s1 = 'café'\n",
    "s2 = 'cafe\\u0301'\n",
    "\n",
    "s3 = 'Straße'\n",
    "s4 = 'strasse'\n",
    "\n",
    "print(nfc_equal(s1, s2))\n",
    "print(nfc_equal('A', 'a'))\n",
    "\n",
    "print(nfc_equal(s3, s4))\n",
    "print(fold_equal(s3, s4))\n",
    "print(fold_equal(s1, s2))\n",
    "print(fold_equal('A', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3 극단적인 '정규화' : 발음 구별 기호 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Herr Voß: • ½ cup of Œtker™ caffe latte • bowl of acai.”\n",
      "Ζεφυρος, Zefiro\n"
     ]
    }
   ],
   "source": [
    "# [예제 4-14] 결합 표시를 모두 제거하는 함수\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def shave_marks(txt):\n",
    "    \"\"\" 발음 구별 기호를 모두 제거 \"\"\"\n",
    "    norm_txt = unicodedata.normalize(\"NFD\", txt) # 모든 문자를 기본 문자와 결합표시로 분해\n",
    "    shaved = ''.join(c for c in norm_txt if not unicodedata.combining(c))\n",
    "    return unicodedata.normalize('NFC', shaved)    \n",
    "\n",
    "order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of açaí.”'\n",
    "print(shave_marks(order))\n",
    "\n",
    "Greek = 'Ζέφυρος, Zéfiro' # 그리스 문자도 변경을 함\n",
    "print(shave_marks(Greek))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Herr Voß: • ½ cup of Œtker™ caffe latte • bowl of acai.”\n",
      "Ζέφυρος, Zefiro\n"
     ]
    }
   ],
   "source": [
    "# [예제 4-16] 라틴 문자에서 결합표시 기호를 제거하는 함수 (예제 4-14 개선)\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def shave_marks(txt):\n",
    "    \"\"\" 라틴식 문자에서 발음 구별 기회를 모두 제거 \"\"\"\n",
    "    norm_txt = unicodedata.normalize('NFD', txt) # 모든 문자를 기본 문자와 결합표시로 분해\n",
    "    \n",
    "    latin_base = False\n",
    "    keepers = []\n",
    "    for c in norm_txt:\n",
    "        if unicodedata.combining(c) and latin_base: # 기반 문자가 라틴 문자일 때만 결합 표시 기호를 건너뜀\n",
    "            continue\n",
    "        keepers.append(c)\n",
    "        if not unicodedata.combining(c): # 결합 문자가 아니라면, 이 문자를 새로운 기반 문자로 간주\n",
    "            latin_base = c in string.ascii_letters # 라틴 문자인지 확인\n",
    "\n",
    "    shaved = ''.join(keepers)\n",
    "    return unicodedata.normalize('NFC', shaved)\n",
    "\n",
    "order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of açaí.”'\n",
    "print(shave_marks(order))\n",
    "\n",
    "Greek = 'Ζέφυρος, Zéfiro' # 라틴 알파벳인 경우만 발음기호를 제거\n",
    "print(shave_marks(Greek))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [예제 4-17] 서양 활자(타이포그래픽) 기회를 아스키로 변환하는 극단적인 바"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Herr Voß: - ½ cup of OEtker(TM) caffè latte - bowl of açaí.\"\n",
      "\"Herr Voss: - 1⁄2 cup of OEtker(TM) caffe latte - bowl of acai.\"\n"
     ]
    }
   ],
   "source": [
    "single_map = str.maketrans(\"\"\"‚ƒ„†ˆ‹‘’“”•–—˜›\"\"\", # 문자대 문자 치환을 위한 매핑 테이블\n",
    " \"\"\"'f\"*^<''\"\"---~>\"\"\")\n",
    "\n",
    "multi_map = str.maketrans({ # 문자대 문자열 치환을 위한 매핑 테이블\n",
    " '€': '<euro>',\n",
    " '…': '...',\n",
    " 'Œ': 'OE',\n",
    " '™': '(TM)',\n",
    " 'œ': 'oe',\n",
    " '‰': '<per mille>',\n",
    " '‡': '**',\n",
    "})\n",
    "\n",
    "multi_map.update(single_map) # 매핑 테이블을 병합\n",
    "\n",
    "def dewinize(txt): # 아스키나 latin1 텍스트에 영향을 미치지 않으며, 마이크로 소프트가 cp1252안의 latin1에 추가한 문자들만 변경\n",
    "    \"\"\"Win1252 기호를 아스키 문자나 시퀀스로 치환한다.\"\"\"\n",
    "    return txt.translate(multi_map)\n",
    "\n",
    "def asciize(txt):\n",
    "    no_marks = shave_marks(dewinize(txt)) # 발음 기호를 제거\n",
    "    no_marks = no_marks.replace('ß', 'ss')\n",
    "    return unicodedata.normalize('NFKC', no_marks) # NFKC를 적용한 호환성 코드 포인트로 대체된 문자열을 만든다.\n",
    "\n",
    "order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of açaí.”'\n",
    "print(dewinize(order)) # 둥근 따옴표, 작은 점, 상표권기호를 대체\n",
    "print(asciize(order))  # 발음 구별 기호를 제거한 후, ß를 ss로 대체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 유니코드 텍스트 정렬하기\n",
    "문자열의 경우 각 단어의 코드 포인트를 비교하지만 아스키 문자가 아닌경우 부적절한 결과가 발생할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acerola', 'atemoia', 'açaí', 'caju', 'cajá']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n",
    "sorted(fruits) # cajá의 순서가 올바르지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬에서 비아스키 텍스트는 locale.strxfrm( ) 함수를 이용해서 변환하는 것이 표준이다. strxfrm() 함수는 문자열을 현지어 비교에 사용할 수 있는 문자열로 변환한다. locale.strxfrm() 함수를 활성화하려면 먼저 애플리케이션에 대해 적절히 현지어를 설정하고 OS가 이 설정을 지원할 수 있어야 한다. 따라서 locale.strxfrm() 함수를 키로 사용하기 전에 setlocale(LC_COLLATE, <지역_언어>)를 호출해야한다. 다음과 같은 점에 주의해야 한다.\n",
    "\n",
    "+ 지역 설정은 시스템 전역에 영향을 미치므로 라이브러리의 setlocale()을 호출하는 것은 권장하지 않는다. 애플리케이션이나 프레임워크는 프로세스를 시작할 때 지역을 설정하고, 그 후에는 변경하면 안된다.\n",
    "+ locale 모듈이 OS에 설치되어 있어야 한다.\n",
    "+ 지역명의 철자를 알고 있어야 한다.  ※ OS마다 이름이 다르며 우분투는 아래를 참고\n",
    "+ OS 제작자에 의해 locale이 올바로 구현되어 있어야 한다. 저자는 우분투에서는 성공했으나 OS_X에서는 실패했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\r\n",
      "C.UTF-8\r\n",
      "en_AG\r\n",
      "en_AG.utf8\r\n",
      "en_AU.utf8\r\n",
      "en_BW.utf8\r\n",
      "en_CA.utf8\r\n",
      "en_DK.utf8\r\n",
      "en_GB.utf8\r\n",
      "en_HK.utf8\r\n",
      "en_IE.utf8\r\n",
      "en_IL\r\n",
      "en_IL.utf8\r\n",
      "en_IN\r\n",
      "en_IN.utf8\r\n",
      "en_NG\r\n",
      "en_NG.utf8\r\n",
      "en_NZ.utf8\r\n",
      "en_PH.utf8\r\n",
      "en_SG.utf8\r\n",
      "en_US.utf8\r\n",
      "en_ZA.utf8\r\n",
      "en_ZM\r\n",
      "en_ZM.utf8\r\n",
      "en_ZW.utf8\r\n",
      "ko_KR.utf8\r\n",
      "POSIX\r\n"
     ]
    }
   ],
   "source": [
    "!locale -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['açaí', 'acerola', 'atemoia', 'cajá', 'caju']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "locale.setlocale(locale.LC_COLLATE, 'ko_KR.utf8') # 컨솔에 locale -a 를 실행했을 때 있는 리스트에서 선택 가능\n",
    "fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n",
    "sorted_fruits = sorted(fruits, key=locale.strxfrm)\n",
    "sorted_fruits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.1 유니코드 대조 알고리즘을 이용한 정렬\n",
    "PyUCA( https://pypi.python.org/pypi/pyuca/ )는 순수 파이썬으로 구현한 유니코드 대조 알고리즘(Unicode Collation Algorithm, UCA)이다. locale.strxfrm() 함수처럼 OS의 영향을 받지 않고 구현도 쉽다. Collator( ) 생성자에 직접 만든 대조 테이블에 대한 경로를 제공하면 된다. 기본적으로는 프로젝트와 함께 제공되는 allkeys.txt( https://github.com/jtauber/pyuca )를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['açaí', 'acerola', 'atemoia', 'cajá', 'caju']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [예제 4-20] pyuca.Collator.sort_Key() 메서드 사용하기\n",
    "import pyuca\n",
    "coll = pyuca.Collator() # 지역 정보를 고려하지 않는다.\n",
    "fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n",
    "sorted_fruits = sorted(fruits, key=coll.sort_key)\n",
    "sorted_fruits \n",
    "# 정렬 방식을 커스터마이즈하려면 Collator() 생성자에 직접 만든 대조 테이블에 대한 경로를 제공한다. 이 테이블은 유니코드 데이터베이스를 구성하는 여러 테이블 중 하나이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 유니코드 데이터베이스\n",
    "유니코드 표준은 수많은 구조화된 텍스트 파일의 형태로 하나의 완전한 데이터베이스를 제공한다. 때문에 문자인지, 십진수인지, 다른 수치형 기호인지를 기록한다. str의 isidentifier(), isprintable(), isdecimal(), isnumeric(), casefold() 메서드는 이 데이터베이스를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U+0031\t    1     \tre_dig\treg_dig\tisdig\tisnum\t 1.00\tDIGIT ONE\n",
      "U+00bc\t    ¼     \t-\t-\t-\tisnum\t 0.25\tVULGAR FRACTION ONE QUARTER\n",
      "U+00b2\t    ²     \t-\t-\tisdig\tisnum\t 2.00\tSUPERSCRIPT TWO\n",
      "U+0969\t    ३     \tre_dig\treg_dig\tisdig\tisnum\t 3.00\tDEVANAGARI DIGIT THREE\n",
      "U+136b\t    ፫     \t-\t-\tisdig\tisnum\t 3.00\tETHIOPIC DIGIT THREE\n",
      "U+216b\t    Ⅻ     \t-\t-\t-\tisnum\t12.00\tROMAN NUMERAL TWELVE\n",
      "U+2466\t    ⑦     \t-\t-\tisdig\tisnum\t 7.00\tCIRCLED DIGIT SEVEN\n",
      "U+2480\t    ⒀     \t-\t-\t-\tisnum\t13.00\tPARENTHESIZED NUMBER THIRTEEN\n",
      "U+3285\t    ㊅     \t-\t-\t-\tisnum\t 6.00\tCIRCLED IDEOGRAPH SIX\n"
     ]
    }
   ],
   "source": [
    "# [예제 4-21] 유니코드 데이터베이스 수치형 문자 메타데이터 사용 예\n",
    "import unicodedata\n",
    "import re\n",
    "import regex\n",
    "\n",
    "re_digit = re.compile(r'\\d')\n",
    "regex_digit = regex.compile(r'\\d')\n",
    "sample = '1\\xbc\\xb2\\u0969\\u136b\\u216b\\u2466\\u2480\\u3285'\n",
    "\n",
    "for char in sample:\n",
    "    print ('U+%04x' % ord(char),\n",
    "           char.center(10), # 문자를 6자리 중 중앙으로 배치\n",
    "           're_dig' if re_digit.match(char) else '-',     # re 모듈은 유니코드를 잘 인식하지 못한다. PyPI를 통해 새로 제공되는\n",
    "           'reg_dig' if regex_digit.match(char) else '-', # regex 모듈은 re모듈을 대체하기 위해 만들어졌으며 유니코드를 더욱 잘 지원한다.(고 한다.)\n",
    "           'isdig' if char.isdigit() else '-', \n",
    "           'isnum' if char.isnumeric() else '-',\n",
    "           format(unicodedata.numeric(char), '5.2f'),\n",
    "           unicodedata.name(char),\n",
    "           sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 이중 모드 str 및 bytes API\n",
    "표준 라이브러리에는 str이나 bytes 인수를 모두 받으며, 인수의 자료형에 따라 다르게 작동하는 함수들이 있다. re와 os모듈이 대표적인 예다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.9.1 정규 표현식에서의 str과 bytes\n",
    "bytes로 정규 표현식을 만들면 \\d와 \\w같은 패턴은 아스키 문자만 매치되지만, str로 이 패턴을 만들면 아스키 문자 이외에 유니코드 숫자나 문자도 매칭된다. 다만 아스키 문자에만 매치아게 만드는 re.ASCII 플래그를 사용하면 str 패턴도 byte와 동일한 결과를 얻을 수 있다. re.compile(pattern, flags=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      " 'Ramanujan saw ௧௭௨௯ as 1729 = 1³ + 12³ = 9³ + 10³'\n",
      "Numbers\n",
      " str: ['௧௭௨௯', '1729', '1', '12', '9', '10']\n",
      " bytes: [b'1729', b'1', b'12', b'9', b'10']\n",
      "Words\n",
      " str: ['Ramanujan', 'saw', '௧௭௨௯', 'as', '1729', '1³', '12³', '9³', '10³']\n",
      " bytes: [b'Ramanujan', b'saw', b'as', b'1729', b'1', b'12', b'9', b'10']\n",
      "Others\n",
      " str numbers(re.ascii): [b'1729', b'1', b'12', b'9', b'10']\n",
      " str words(re.ascii): [b'Ramanujan', b'saw', b'as', b'1729', b'1', b'12', b'9', b'10']\n"
     ]
    }
   ],
   "source": [
    "# [예제 4-22] ramanujan.py: 간단한 str과 bytes 정규 표현식의 동작 비교\n",
    "import re\n",
    "\n",
    "re_numbers_str = re.compile(r'\\d+') # str 형\n",
    "re_words_str = re.compile(r'\\w+')   # str 형\n",
    "re_numbers_bytes = re.compile(b'\\d+') # bytes 형\n",
    "re_words_bytes = re.compile(b'\\w+')   # bytes 형\n",
    "\n",
    "re_words_str_asbytes1 = re.compile(b'\\d+', re.ASCII)   # str 형\n",
    "re_words_str_asbytes2 = re.compile(b'\\w+', re.ASCII)   # str 형\n",
    "\n",
    "text_str = (\"Ramanujan saw \\u0be7\\u0bed\\u0be8\\u0bef\"\n",
    "           \" as 1729 = 1³ + 12³ = 9³ + 10³\")\n",
    "\n",
    "text_bytes = text_str.encode('utf_8')\n",
    "\n",
    "\"\"\" 정규 표현식을 str과 bytes에 사용할 수 있지만 바이트에 정규식을 사용하면 아스키 범위를 벗어나는 문자들은 숫자나 단어로 처리하지 않는다. \"\"\"\n",
    "print('Text', repr(text_str), sep='\\n ')\n",
    "print('Numbers')\n",
    "print(' str:', re_numbers_str.findall(text_str))      # 타밀과 아스키 숫자에 매칭\n",
    "print(' bytes:', re_numbers_bytes.findall(text_bytes)) # 아스키 숫자에만 매칭\n",
    "print('Words')\n",
    "print(' str:', re_words_str.findall(text_str))        # 문자, 위첨자, 타밀, 아스키 숫자에 매칭\n",
    "print(' bytes:', re_words_bytes.findall(text_bytes))   # 문자와 숫자에 대한 아스키 바이트에만 매칭\n",
    "print('Others')\n",
    "print(' str numbers(re.ascii):', re_words_str_asbytes1.findall(text_bytes))   \n",
    "print(' str words(re.ascii):', re_words_str_asbytes2.findall(text_bytes))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.9.2 os 모듈 함수에서 str과 bytes\n",
    "다양한 운영체계를 클라이언트로 가지는 파일 서버는 인코딩 할 수 없는 파일명을 가질 수 있다. 이 문제를 해결하기 위해 파일명이라 경로명을 받는 모든 os 모듈 함수는 str이나 bytes 형의 인수를 받는다. \n",
    "\n",
    "str인수로 호출하는 경우에는 sys.getfilesystemencoding() 함수에 의해 지정된 코덱을 이용해서 자동변환되고 운영 체계의 응답은 동일 코덱을 이용해서 디코딩된다. 대부분의 경우 이 방법은 유니코드 샌드위치 모델에 따라 여러분이 원하는 대로 작동한다. \n",
    "\n",
    "그렇지만 이렇게 처리할 수 없는 파일명을 다루거나 수정해야 할 때는 bytes 인수를 os함수에 전달해서 bytes 변환값을 가져올 우 있다. 파일명이나 경로명에 얼마나 많은 깨진 문자가 있는지 상관없이 이 방식을 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wave.gif', 'ch12_InheritanceForGoodOrForWorse.ipynb', 'unicodesandwich.png', 'ch11_tombola.py', 'ch5_FirstLevelFunction.ipynb', 'ch6_1stLevelFunctionDesignPattern.ipynb', 'Figure12-1.png', 'is_a_relation.png', 'bingo.py', '.git', 'ch3_DictionaryAndSet.ipynb', 'ch2_Sequence.ipynb', 'ch5_bobo_sample.py', 'ch11_tombola_runner.py', 'ch11_Interfaces_FromProtocolsToABCs.ipynb', 'ch4_TextAndBytes.ipynb', 'Figure8-4.png', 'ch9_PythonicObject.ipynb', 'ch1_PythonDataModel.ipynb', 'ch11_tombolist.py', 'cafe.txt', 'ch10_SequenceHacking_Hash_Slice.ipynb', 'ch14_IterablesIteratorsAndGenerators.ipynb', 'ch7_DecoratorAndCloser.ipynb', 'Figure8-1.png', 'ch11_lotto.py', '.ipynb_checkpoints', 'ch11_drum.py', 'tombola_tests.rst', 'dummy', 'README.md', 'ch8_ObjectReferences_mutability_recycling.ipynb']\n",
      "[b'wave.gif', b'ch12_InheritanceForGoodOrForWorse.ipynb', b'unicodesandwich.png', b'ch11_tombola.py', b'ch5_FirstLevelFunction.ipynb', b'ch6_1stLevelFunctionDesignPattern.ipynb', b'Figure12-1.png', b'is_a_relation.png', b'bingo.py', b'.git', b'ch3_DictionaryAndSet.ipynb', b'ch2_Sequence.ipynb', b'ch5_bobo_sample.py', b'ch11_tombola_runner.py', b'ch11_Interfaces_FromProtocolsToABCs.ipynb', b'ch4_TextAndBytes.ipynb', b'Figure8-4.png', b'ch9_PythonicObject.ipynb', b'ch1_PythonDataModel.ipynb', b'ch11_tombolist.py', b'cafe.txt', b'ch10_SequenceHacking_Hash_Slice.ipynb', b'ch14_IterablesIteratorsAndGenerators.ipynb', b'ch7_DecoratorAndCloser.ipynb', b'Figure8-1.png', b'ch11_lotto.py', b'.ipynb_checkpoints', b'ch11_drum.py', b'tombola_tests.rst', b'dummy', b'README.md', b'ch8_ObjectReferences_mutability_recycling.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('.'))\n",
    "print(os.listdir(b'.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_name_bytes = b'digits-of-\\xcf\\x80.txt'\n",
    "pi_name_str = pi_name_bytes.decode('ascii', 'surrogateescape') # ascii로 디코딩 할 수 없는 바이트 유니코드 표준에서 '하위 써로케이트 영역'이라고 하는 U+DC00 ~ U+DCFF까지의 코드 포인트로 치환한다.\n",
    "print(pi_name_str)\n",
    "print(pi_name_str.encode('ascii', 'surrogateescape')) # 책과 달리 에러가 나서 당황;;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
